# Utils/TokenizationUtil.py
# -*- coding: utf-8 -*-

import re
import torch
from typing import List, Optional, Sequence

from Utils import Util

# ---------- compat: make Util.text_to_token_ids accept device & return 2D LongTensor ----------
def _patch_util_text_to_token_ids():
    import inspect
    try:
        sig = inspect.signature(Util.text_to_token_ids)
        if "device" in sig.parameters:
            return  # already compatible
    except Exception:
        pass

    _orig = Util.text_to_token_ids

    def _compat(text, tokenizer, device=None):
        ids = _orig(text, tokenizer)
        if isinstance(ids, torch.Tensor):
            t = ids
        else:
            t = torch.tensor(ids, dtype=torch.long)
        if t.dim() == 1:
            t = t.unsqueeze(0)
        if device is not None:
            t = t.to(device)
        return t

    Util.text_to_token_ids = _compat

_patch_util_text_to_token_ids()
# ---------------------------------------------------------------------------------------------


# ---------- tokenizer wrapper (ignore allowed_special; ensure pad_token) ----------
class _TokenizerCompat:
    def __init__(self, tok):
        self._tok = tok
        if getattr(self._tok, "pad_token_id", None) is None and getattr(self._tok, "eos_token", None) is not None:
            self._tok.pad_token = self._tok.eos_token

    def encode(self, text, *args, **kwargs):
        kwargs.pop("allowed_special", None)
        return self._tok.encode(text, *args, **kwargs)

    def decode(self, *args, **kwargs):
        return self._tok.decode(*args, **kwargs)

    def __getattr__(self, name):
        return getattr(self._tok, name)


def wrap_hf_tokenizer(tokenizer):
    return _TokenizerCompat(tokenizer)
# --------------------------------------------------------------------------------


# ---------- stop & trimming helpers ----------
def ensure_response_suffix(prompt: str, suffix: str = "### Response:\n") -> str:
    if prompt.endswith(suffix):
        return prompt
    if not prompt.endswith("\n"):
        prompt += "\n"
    return prompt + suffix


def prepare_stop_ids(
    stop_sequences: Optional[Sequence[str]],
    tokenizer,
    device: torch.device
) -> List[torch.Tensor]:
    out: List[torch.Tensor] = []
    if not stop_sequences:
        return out
    for s in stop_sequences:
        ids = tokenizer.encode(s)
        if ids:
            out.append(torch.tensor(ids, dtype=torch.long, device=device))
    return out


def find_first_stop_after(
    seq_1d: torch.Tensor,
    start_idx: int,
    eos_id: Optional[int],
    stop_ids: Sequence[torch.Tensor]
) -> int:
    cut = seq_1d.numel()

    if eos_id is not None:
        pos = (seq_1d == eos_id).nonzero(as_tuple=False)
        if pos.numel() > 0:
            p = int(pos[0].item())
            if p >= start_idx:
                cut = min(cut, p)

    for sid in stop_ids:
        L = int(sid.numel())
        if L == 0 or seq_1d.numel() < start_idx + L:
            continue
        for i in range(start_idx, seq_1d.numel() - L + 1):
            if torch.equal(seq_1d[i:i+L], sid):
                cut = min(cut, i)
                break

    return cut


def trim_after_prompt(
    seq_1d: torch.Tensor,
    prompt_len: int,
    eos_id: Optional[int],
    stop_ids: Sequence[torch.Tensor]
) -> torch.Tensor:
    cut = find_first_stop_after(seq_1d, prompt_len, eos_id, stop_ids)
    if cut <= prompt_len:
        return seq_1d.new_zeros((0,), dtype=torch.long)
    return seq_1d[prompt_len:cut]


def safe_decode_after_trim(
    full_seq,
    prompt_len,
    eos_id,
    stop_ids,
    tokenizer,
    fallback_tokens=40,
    fallback_max_tokens=160,
):
    """
    Decode tokens after prompt. If trimmed result is empty or whitespace/special-only,
    progressively widen a fallback window so we return *something* generated by the model.
    (No .strip() here — caller decides how to trim.)
    """
    answer_tokens = trim_after_prompt(full_seq, prompt_len, eos_id, stop_ids)

    def _decode(toks):
        return tokenizer.decode(
            toks.tolist(),
            skip_special_tokens=False,
            clean_up_tokenization_spaces=False,
        )

    if answer_tokens.numel() > 0:
        txt = _decode(answer_tokens)
        if txt:
            return txt

    total_new = int(full_seq.numel() - prompt_len)
    if total_new <= 0:
        return ""

    max_take = min(fallback_max_tokens, total_new)
    step = max(1, fallback_tokens)
    for n in range(step, max_take + 1, step):
        tail = full_seq[prompt_len : prompt_len + n]
        txt = _decode(tail)
        if re.search(r"[A-Za-z0-9]", txt):
            return txt

    tail = full_seq[prompt_len : prompt_len + max_take]
    return _decode(tail)


def decode_and_trim_text_markers(
    text: str,
    text_markers: Optional[Sequence[str]] = None,
    min_keep_chars: int = 20
) -> str:
    """
    Soft trim at the earliest marker *only if* it appears after `min_keep_chars`.
    If trimming would produce empty/whitespace, keep a small visible prefix instead.
    """
    if not text_markers:
        s = text.strip()
        return s if s else text[:min_keep_chars]

    earliest = None
    for m in text_markers:
        i = text.find(m)
        if i != -1:
            earliest = i if earliest is None else min(earliest, i)

    if earliest is None or earliest < min_keep_chars:
        s = text.strip()
        return s if s else text[:min_keep_chars]

    kept = text[:earliest]
    kept_stripped = kept.strip()
    if kept_stripped:
        return kept_stripped
    return text[:max(min_keep_chars, 20)]


def drop_instruction_echo(response_text: str, instruction: str, min_inst_len: int = 10) -> str:
    inst = (instruction or "").strip()
    if not inst or len(inst) < min_inst_len:
        return response_text
    pattern = r"^\s*" + re.escape(inst) + r"[\s\.:,-]*"
    return re.sub(pattern, "", response_text, count=1)
# --------------------------------------------------------------------------

# Collect token ids that decode to only whitespace/newlines/tabs, so we can ban
# them for the first few decoding steps in greedy.
def collect_leading_whitespace_token_ids(tokenizer):
    samples = [
        " ", "\t", "\n", "\r", "\n\n", "\t\t", "  ", "   ", "\n \n", "\n\t", "\t\n",
    ]
    ids = set()
    for s in samples:
        try:
            enc = tokenizer.encode(s)
        except Exception:
            enc = []
        for tid in enc:
            try:
                dec = tokenizer.decode([tid], skip_special_tokens=False, clean_up_tokenization_spaces=False)
            except Exception:
                dec = ""
            if dec and dec.strip("") == dec.strip():  # avoid exceptions; we just check below
                pass
            # Keep if it’s ONLY whitespace
            if dec != "" and dec.strip() == "":
                ids.add(int(tid))
    return sorted(ids)
